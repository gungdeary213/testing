# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14cB14EYz6MnxC4IaPpUnVJvSVGN3N6eG
"""
#import Library
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import cross_val_score
import pickle
import warnings
warnings.filterwarnings('ignore')

# Set random seed untuk reproducibility
np.random.seed(42)

class PropertyPricePredictor:
    def __init__(self):
        self.scaler_X = StandardScaler()
        self.scaler_y = StandardScaler()
        self.models = {}
        self.best_model = None

    def generate_synthetic_data(self, n_samples=200):
        """Generate synthetic property price data dengan hubungan non-linear"""
        print("Generating synthetic property data...")

        # Generate fitur dengan korelasi realistis
        luas_tanah = np.random.uniform(50, 500, n_samples)
        luas_bangunan = np.random.uniform(30, 400, n_samples)
        kamar_tidur = np.random.randint(1, 6, n_samples)
        umur_bangunan = np.random.uniform(0, 30, n_samples)
        jarak_pusat = np.random.uniform(1, 20, n_samples)

        # Buat hubungan non-linear dengan harga
        # Base price dengan hubungan polynomial
        base_price = (
            200 +  # base price
            5 * luas_tanah + 0.1 * luas_tanah**2 +  # quadratic relationship dengan luas tanah
            8 * luas_bangunan + 0.05 * luas_bangunan**2 +  # quadratic relationship dengan luas bangunan
            150 * kamar_tidur**1.5 +  # non-linear dengan kamar tidur
            -20 * umur_bangunan + 0.5 * umur_bangunan**2 +  # quadratic dengan umur (older initially cheaper, then maintenance cost)
            -80 * jarak_pusat + 2 * jarak_pusat**2 +  # quadratic dengan jarak
            np.random.normal(0, 100, n_samples)  # noise
        )

        # Tambahkan interaksi antara fitur
        interaction_terms = (
            0.1 * luas_tanah * luas_bangunan +
            25 * kamar_tidur * luas_bangunan +
            -5 * umur_bangunan * jarak_pusat
        )

        harga_properti = np.maximum(200, base_price + interaction_terms)

        # Buat DataFrame
        data = pd.DataFrame({
            'luas_tanah': luas_tanah,
            'luas_bangunan': luas_bangunan,
            'kamar_tidur': kamar_tidur,
            'umur_bangunan': umur_bangunan,
            'jarak_pusat': jarak_pusat,
            'harga_properti': harga_properti
        })

        data.to_csv('dataset_price.csv', index=False)
        print(" Dataset berhasil disimpan sebagai dataset_price.csv")

        print(data.head())

        return data

    def exploratory_data_analysis(self, data):
        """Melakukan exploratory data analysis"""
        print("\n=== EXPLORATORY DATA ANALYSIS ===\n")

        # Statistical Summary
        print("Statistical Summary:")
        print(data.describe())

        # Visualisasi distribusi setiap fitur
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        features = ['luas_tanah', 'luas_bangunan', 'kamar_tidur', 'umur_bangunan', 'jarak_pusat', 'harga_properti']

        for i, feature in enumerate(features):
            ax = axes[i//3, i%3]
            data[feature].hist(bins=20, ax=ax, alpha=0.7)
            ax.set_title(f'Distribusi {feature}')
            ax.set_xlabel(feature)
            ax.set_ylabel('Frekuensi')

        plt.tight_layout()
        plt.show()

        # Scatter plot setiap fitur vs harga
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        for i, feature in enumerate(features[:-1]):  # Exclude target
            ax = axes[i//3, i%3]
            ax.scatter(data[feature], data['harga_properti'], alpha=0.6)
            ax.set_xlabel(feature)
            ax.set_ylabel('Harga Properti')
            ax.set_title(f'{feature} vs Harga')

        plt.tight_layout()
        plt.show()

        # Correlation matrix
        plt.figure(figsize=(10, 8))
        correlation_matrix = data.corr()
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
        plt.title('Correlation Matrix')
        plt.show()

        # Identifikasi outliers dengan boxplot
        plt.figure(figsize=(12, 8))
        data_boxplot = data.drop('harga_properti', axis=1)
        data_boxplot.boxplot()
        plt.title('Boxplot untuk Identifikasi Outliers')
        plt.xticks(rotation=45)
        plt.show()

        return correlation_matrix

    def preprocess_data(self, data):
        """Preprocessing data untuk training"""
        print("\n=== DATA PREPROCESSING ===\n")

        # Split features dan target
        X = data.drop('harga_properti', axis=1)
        y = data['harga_properti']

        # Split data train-test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )

        print(f"Training set size: {X_train.shape[0]}")
        print(f"Testing set size: {X_test.shape[0]}")

        # Feature scaling
        X_train_scaled = self.scaler_X.fit_transform(X_train)
        X_test_scaled = self.scaler_X.transform(X_test)

        # Target scaling (opsional, tapi membantu untuk regression)
        y_train_scaled = self.scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()
        y_test_scaled = self.scaler_y.transform(y_test.values.reshape(-1, 1)).flatten()

        return X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, X_train, X_test, y_train, y_test

    def train_models(self, X_train_scaled, y_train_scaled):
        """Melatih beberapa model regresi"""
        print("\n=== MODEL TRAINING ===\n")

        # Linear Regression
        lr_model = LinearRegression()
        lr_model.fit(X_train_scaled, y_train_scaled)
        self.models['Linear Regression'] = lr_model
        print("Linear Regression model trained.")

        # Ridge Regression
        ridge_model = Ridge(alpha=1.0)  # Regularization parameter
        ridge_model.fit(X_train_scaled, y_train_scaled)
        self.models['Ridge Regression'] = ridge_model
        print("Ridge Regression model trained.")

        # Lasso Regression
        lasso_model = Lasso(alpha=0.1)  # Regularization parameter
        lasso_model.fit(X_train_scaled, y_train_scaled)
        self.models['Lasso Regression'] = lasso_model
        print("Lasso Regression model trained.")

    def evaluate_models(self, X_test_scaled, y_test_scaled, y_test):
        """Mengevaluasi model dan memilih yang terbaik"""
        print("\n=== MODEL EVALUATION ===\n")

        results = {}
        for name, model in self.models.items():
            y_pred_scaled = model.predict(X_test_scaled)

            # Inverse transform predictions to original scale for evaluation
            y_pred = self.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()

            # Calculate evaluation metrics
            r2 = r2_score(y_test, y_pred)
            mse = mean_squared_error(y_test, y_pred)
            rmse = np.sqrt(mse)
            mae = mean_absolute_error(y_test, y_pred)

            results[name] = {'R2': r2, 'MSE': mse, 'RMSE': rmse, 'MAE': mae}
            print(f"{name} - R2: {r2:.4f}, MSE: {mse:.2f}, RMSE: {rmse:.2f}, MAE: {mae:.2f}")

        # Pilih model terbaik berdasarkan R2 score
        self.best_model_name = max(results, key=lambda k: results[k]['R2'])
        self.best_model = self.models[self.best_model_name]
        print(f"\nBest model selected: {self.best_model_name}")

        return results

    def predict(self, new_data):
        """Melakukan prediksi menggunakan model terbaik"""
        if self.best_model is None:
            print("No model has been trained yet. Please train models first.")
            return None

        # Ensure new_data is a DataFrame
        if isinstance(new_data, dict):
            new_data = pd.DataFrame([new_data])
        elif not isinstance(new_data, pd.DataFrame):
            print("Input data must be a dictionary or pandas DataFrame.")
            return None

        # Scale new data using the fitted scaler
        new_data_scaled = self.scaler_X.transform(new_data)

        # Predict using the best model
        prediction_scaled = self.best_model.predict(new_data_scaled)

        # Inverse transform the prediction to original scale
        prediction = self.scaler_y.inverse_transform(prediction_scaled.reshape(-1, 1)).flatten()

        return prediction

    def save_model(self, filename="property_price_predictor.pkl"):
        """Menyimpan model terbaik dan scalers"""
        if self.best_model is None:
            print("No model has been trained yet. Cannot save.")
            return

        try:
            with open(filename, 'wb') as f:
                pickle.dump({
                    'model': self.best_model,
                    'scaler_X': self.scaler_X,
                    'scaler_y': self.scaler_y,
                    'best_model_name': self.best_model_name
                }, f)
            print(f"Model and scalers saved successfully to {filename}")
        except Exception as e:
            print(f"Error saving model: {e}")

    def load_model(self, filename="property_price_predictor.pkl"):
        """Memuat model terbaik dan scalers"""
        try:
            with open(filename, 'rb') as f:
                data = pickle.load(f)
                self.best_model = data['model']
                self.scaler_X = data['scaler_X']
                self.scaler_y = data['scaler_y']
                self.best_model_name = data['best_model_name']
            print(f"Model and scalers loaded successfully from {filename}")
        except FileNotFoundError:
            print(f"Error loading model: File '{filename}' not found.")
        except Exception as e:
            print(f"Error loading model: {e}")

# Contoh penggunaan kelas
if __name__ == '__main__':
    predictor = PropertyPricePredictor()

    # 1. Generate data
    data = predictor.generate_synthetic_data()

    # 2. EDA (Optional)
    # correlation_matrix = predictor.exploratory_data_analysis(data)

    # 3. Preprocess data
    X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, X_train, X_test, y_train, y_test = predictor.preprocess_data(data)

    # 4. Train models
    predictor.train_models(X_train_scaled, y_train_scaled)

    # 5. Evaluate models
    results = predictor.evaluate_models(X_test_scaled, y_test_scaled, y_test)

    # 6. Make a prediction
    new_property_data = {
        'luas_tanah': 300,
        'luas_bangunan': 250,
        'kamar_tidur': 4,
        'umur_bangunan': 5,
        'jarak_pusat': 8
    }
    predicted_price = predictor.predict(new_property_data)
    print(f"\nPredicted price for the new property: ${predicted_price[0]:,.2f}")

    # 7. Save the model
    predictor.save_model("my_property_predictor.pkl")

    # 8. Load the model and make another prediction
    loaded_predictor = PropertyPricePredictor()
    loaded_predictor.load_model("my_property_predictor.pkl")

    new_property_data_2 = pd.DataFrame({
        'luas_tanah': [450],
        'luas_bangunan': [350],
        'kamar_tidur': [5],
        'umur_bangunan': [10],
        'jarak_pusat': [3]
    })

    predicted_price_2 = loaded_predictor.predict(new_property_data_2)
    if predicted_price_2 is not None:
        print(f"\nPredicted price for the second new property (using loaded model): ${predicted_price_2[0]:,.2f}")

class ModelImplementation:
    def __init__(self, scaler_X, scaler_y):
        self.scaler_X = scaler_X
        self.scaler_y = scaler_y
        self.models = {}
        self.poly_features = {}

    def create_polynomial_features(self, X_train, X_test, degrees):
        """Membuat polynomial features untuk berbagai degree"""
        print("\n=== POLYNOMIAL FEATURE ENGINEERING ===\n")

        poly_data = {}

        for degree in degrees:
            poly = PolynomialFeatures(degree=degree, include_bias=False)
            X_train_poly = poly.fit_transform(X_train)
            X_test_poly = poly.transform(X_test)

            poly_data[degree] = {
                'train': X_train_poly,
                'test': X_test_poly,
                'transformer': poly
            }

            print(f"Degree {degree}: {X_train_poly.shape[1]} features")

        self.poly_features = poly_data
        return poly_data

    def train_models(self, X_train_poly, X_test_poly, y_train, y_test, degree):
        """Train berbagai model untuk degree tertentu"""
        print(f"\n--- Training Models for Degree {degree} ---")

        degree_models = {}

        # Linear Regression
        print("Training Linear Regression...")
        lr = LinearRegression()
        lr.fit(X_train_poly, y_train)
        degree_models['Linear'] = lr

        # Ridge Regression dengan berbagai alpha
        alphas = [0.1, 1, 10]
        for alpha in alphas:
            print(f"Training Ridge Regression (alpha={alpha})...")
            ridge = Ridge(alpha=alpha, random_state=42)
            ridge.fit(X_train_poly, y_train)
            degree_models[f'Ridge_alpha_{alpha}'] = ridge

        # Lasso Regression dengan berbagai alpha
        for alpha in alphas:
            print(f"Training Lasso Regression (alpha={alpha})...")
            lasso = Lasso(alpha=alpha, random_state=42)
            lasso.fit(X_train_poly, y_train)
            degree_models[f'Lasso_alpha_{alpha}'] = lasso

        self.models[degree] = degree_models
        return degree_models

    def custom_polynomial_regression(self, X_train, y_train, X_test, y_test, degree=2, learning_rate=0.01, epochs=1000):
        """Implementasi custom polynomial regression dari scratch"""
        print(f"\n--- Custom Polynomial Regression (Degree {degree}) ---")

        # Add bias term
        X_train_bias = np.c_[np.ones(X_train.shape[0]), X_train]
        X_test_bias = np.c_[np.ones(X_test.shape[0]), X_test]

        # Initialize weights
        theta = np.random.randn(X_train_bias.shape[1])

        # Gradient descent
        m = len(y_train)
        losses = []

        for epoch in range(epochs):
            # Predictions
            predictions = X_train_bias.dot(theta)

            # Calculate error
            error = predictions - y_train

            # Calculate gradient
            gradient = (1/m) * X_train_bias.T.dot(error)

            # Update weights
            theta = theta - learning_rate * gradient

            # Calculate loss
            loss = np.mean(error**2)
            losses.append(loss)

            if epoch % 200 == 0:
                print(f"Epoch {epoch}, Loss: {loss:.4f}")

        # Final predictions
        train_predictions = X_train_bias.dot(theta)
        test_predictions = X_test_bias.dot(theta)

        # Calculate metrics
        train_r2 = r2_score(y_train, train_predictions)
        test_r2 = r2_score(y_test, test_predictions)

        print(f"Custom Model - Train R²: {train_r2:.4f}, Test R²: {test_r2:.4f}")

        return {
            'theta': theta,
            'train_predictions': train_predictions,
            'test_predictions': test_predictions,
            'losses': losses,
            'train_r2': train_r2,
            'test_r2': test_r2
        }

class ModelEvaluator:
    def __init__(self, scaler_y):
        self.scaler_y = scaler_y

    def calculate_metrics(self, y_true, y_pred, dataset_type='Test'):
        """Menghitung berbagai metrics evaluasi"""
        y_true_original = self.scaler_y.inverse_transform(y_true.reshape(-1, 1)).flatten()
        y_pred_original = self.scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()

        r2 = r2_score(y_true_original, y_pred_original)
        mse = mean_squared_error(y_true_original, y_pred_original)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_true_original, y_pred_original)
        mape = np.mean(np.abs((y_true_original - y_pred_original) / y_true_original)) * 100

        metrics = {
            f'{dataset_type} R²': r2,
            f'{dataset_type} MSE': mse,
            f'{dataset_type} RMSE': rmse,
            f'{dataset_type} MAE': mae,
            f'{dataset_type} MAPE': mape
        }

        return metrics

    def evaluate_all_models(self, models, poly_features, y_train, y_test):
        """Evaluasi semua model yang telah di-training"""
        print("\n=== MODEL EVALUATION ===\n")

        results = []

        for degree, degree_models in models.items():
            X_train_poly = poly_features[degree]['train']
            X_test_poly = poly_features[degree]['test']

            for model_name, model in degree_models.items():
                # Predictions
                y_train_pred = model.predict(X_train_poly)
                y_test_pred = model.predict(X_test_poly)

                # Calculate metrics
                train_metrics = self.calculate_metrics(y_train, y_train_pred, 'Train')
                test_metrics = self.calculate_metrics(y_test, y_test_pred, 'Test')

                # Combine results
                model_result = {
                    'Degree': degree,
                    'Model': model_name,
                    **train_metrics,
                    **test_metrics
                }

                results.append(model_result)

                print(f"Degree {degree} - {model_name}:")
                print(f"  Train R²: {train_metrics['Train R²']:.4f}, Test R²: {test_metrics['Test R²']:.4f}")
                print(f"  Train RMSE: {train_metrics['Train RMSE']:.2f}, Test RMSE: {test_metrics['Test RMSE']:.2f}")

        results_df = pd.DataFrame(results)
        return results_df

    def plot_model_comparison(self, results_df):
        """Plot perbandingan performa model"""
        # R² Score comparison
        plt.figure(figsize=(15, 5))

        plt.subplot(1, 2, 1)
        for model_type in results_df['Model'].unique():
            if 'Linear' in model_type:
                continue
            model_data = results_df[results_df['Model'] == model_type]
            plt.plot(model_data['Degree'], model_data['Test R²'], marker='o', label=model_type)

        plt.xlabel('Polynomial Degree')
        plt.ylabel('Test R² Score')
        plt.title('Test R² Score vs Polynomial Degree')
        plt.legend()
        plt.grid(True)

        # RMSE comparison
        plt.subplot(1, 2, 2)
        for model_type in results_df['Model'].unique():
            if 'Linear' in model_type:
                continue
            model_data = results_df[results_df['Model'] == model_type]
            plt.plot(model_data['Degree'], model_data['Test RMSE'], marker='o', label=model_type)

        plt.xlabel('Polynomial Degree')
        plt.ylabel('Test RMSE')
        plt.title('Test RMSE vs Polynomial Degree')
        plt.legend()
        plt.grid(True)

        plt.tight_layout()
        plt.show()

    def plot_residuals(self, models, poly_features, X_test, y_test, best_model_info):
        """Plot residual analysis"""
        degree, model_name, model = best_model_info
        X_test_poly = poly_features[degree]['test']

        y_test_pred = model.predict(X_test_poly)
        y_test_original = self.scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()
        y_test_pred_original = self.scaler_y.inverse_transform(y_test_pred.reshape(-1, 1)).flatten()

        residuals = y_test_original - y_test_pred_original

        fig, axes = plt.subplots(1, 2, figsize=(15, 6))

        # Residuals vs Predicted
        axes[0].scatter(y_test_pred_original, residuals, alpha=0.6)
        axes[0].axhline(y=0, color='red', linestyle='--')
        axes[0].set_xlabel('Predicted Values')
        axes[0].set_ylabel('Residuals')
        axes[0].set_title('Residuals vs Predicted Values')

        # Predicted vs Actual
        axes[1].scatter(y_test_original, y_test_pred_original, alpha=0.6)
        axes[1].plot([y_test_original.min(), y_test_original.max()],
                    [y_test_original.min(), y_test_original.max()], 'red', linestyle='--')
        axes[1].set_xlabel('Actual Values')
        axes[1].set_ylabel('Predicted Values')
        axes[1].set_title('Predicted vs Actual Values')

        plt.tight_layout()
        plt.show()

class RegularizationAnalyzer:
    def __init__(self, scaler_y):
        self.scaler_y = scaler_y

    def analyze_regularization(self, X_train, X_test, y_train, y_test, degrees):
        """Analisis efek regularization dengan berbagai alpha"""
        print("\n=== REGULARIZATION ANALYSIS ===\n")

        alphas = [0.001, 0.01, 0.1, 1, 10, 100]
        results = []

        for degree in degrees:
            # Create polynomial features
            poly = PolynomialFeatures(degree=degree, include_bias=False)
            X_train_poly = poly.fit_transform(X_train)
            X_test_poly = poly.transform(X_test)

            for alpha in alphas:
                # Ridge Regression
                ridge = Ridge(alpha=alpha, random_state=42)
                ridge.fit(X_train_poly, y_train)
                ridge_test_r2 = r2_score(
                    self.scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten(),
                    self.scaler_y.inverse_transform(ridge.predict(X_test_poly).reshape(-1, 1)).flatten()
                )

                # Lasso Regression
                lasso = Lasso(alpha=alpha, random_state=42)
                lasso.fit(X_train_poly, y_train)
                lasso_test_r2 = r2_score(
                    self.scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten(),
                    self.scaler_y.inverse_transform(lasso.predict(X_test_poly).reshape(-1, 1)).flatten()
                )

                results.append({
                    'Degree': degree,
                    'Alpha': alpha,
                    'Ridge_R2': ridge_test_r2,
                    'Lasso_R2': lasso_test_r2
                })

        results_df = pd.DataFrame(results)

        # Plot results
        plt.figure(figsize=(15, 10))

        for i, degree in enumerate(degrees):
            plt.subplot(2, 3, i+1)
            degree_data = results_df[results_df['Degree'] == degree]

            plt.plot(degree_data['Alpha'], degree_data['Ridge_R2'], marker='o', label='Ridge')
            plt.plot(degree_data['Alpha'], degree_data['Lasso_R2'], marker='s', label='Lasso')

            plt.xscale('log')
            plt.xlabel('Alpha (log scale)')
            plt.ylabel('Test R² Score')
            plt.title(f'Degree {degree} - Regularization Effect')
            plt.legend()
            plt.grid(True)

        plt.tight_layout()
        plt.show()

        return results_df

    def analyze_feature_importance(self, models, poly_features, feature_names):
        """Analisis feature importance dari model"""
        print("\n=== FEATURE IMPORTANCE ANALYSIS ===\n")

        for degree, degree_models in models.items():
            if degree > 3:  # Batasi untuk degree yang tidak terlalu tinggi
                continue

            poly_transformer = poly_features[degree]['transformer']
            feature_names_poly = poly_transformer.get_feature_names_out(feature_names)

            print(f"\n--- Degree {degree} Feature Importance ---")

            for model_name, model in degree_models.items():
                if 'Linear' in model_name or 'Ridge' in model_name:
                    coefficients = model.coef_

                    # Get top 10 most important features
                    feature_importance = pd.DataFrame({
                        'Feature': feature_names_poly,
                        'Coefficient': coefficients
                    }).sort_values('Coefficient', key=abs, ascending=False).head(10)

                    print(f"\n{model_name} - Top 10 Features:")
                    print(feature_importance)

                    # Plot feature importance
                    plt.figure(figsize=(10, 6))
                    colors = ['red' if coef < 0 else 'blue' for coef in feature_importance['Coefficient']]
                    plt.barh(feature_importance['Feature'], feature_importance['Coefficient'], color=colors)
                    plt.xlabel('Coefficient Value')
                    plt.title(f'Degree {degree} - {model_name} - Feature Importance')
                    plt.tight_layout()
                    plt.show()

class ModelSelector:
    def __init__(self, scaler_X, scaler_y):
        self.scaler_X = scaler_X
        self.scaler_y = scaler_y

    def select_best_model(self, results_df, models, poly_features, X_train, y_train):
        """Memilih model terbaik berdasarkan berbagai metrics"""
        print("\n=== BEST MODEL SELECTION ===\n")

        # Filter untuk model dengan performance terbaik
        best_test_r2 = results_df.loc[results_df['Test R²'].idxmax()]
        best_test_rmse = results_df.loc[results_df['Test RMSE'].idxmin()]

        print("Best Model by Test R²:")
        print(best_test_r2[['Degree', 'Model', 'Train R²', 'Test R²']])

        print("\nBest Model by Test RMSE:")
        print(best_test_rmse[['Degree', 'Model', 'Train RMSE', 'Test RMSE']])

        # Pilih model dengan balance terbaik antara train dan test performance
        results_df['Train_Test_Diff'] = abs(results_df['Train R²'] - results_df['Test R²'])
        balanced_model = results_df.loc[results_df['Train_Test_Diff'].idxmin()]

        print("\nMost Balanced Model (Smallest Train-Test Difference):")
        print(balanced_model[['Degree', 'Model', 'Train R²', 'Test R²', 'Train_Test_Diff']])

        # Cross-validation untuk konfirmasi
        best_degree = int(balanced_model['Degree'])
        best_model_name = balanced_model['Model']
        best_model = models[best_degree][best_model_name]

        poly_transformer = poly_features[best_degree]['transformer']
        X_train_poly = poly_transformer.transform(X_train)

        # Cross-validation score
        cv_scores = cross_val_score(best_model, X_train_poly, y_train, cv=5, scoring='r2')
        print(f"\nCross-Validation Scores: {cv_scores}")
        print(f"Mean CV R²: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

        best_model_info = {
            'degree': best_degree,
            'model_name': best_model_name,
            'model': best_model,
            'poly_transformer': poly_transformer,
            'cv_score': cv_scores.mean()
        }

        return best_model_info

    def make_predictions(self, best_model_info, X_test, y_test):
        """Membuat prediksi dengan model terbaik"""
        print("\n=== FINAL PREDICTIONS ===\n")

        degree = best_model_info['degree']
        model = best_model_info['model']
        poly_transformer = best_model_info['poly_transformer']

        # Transform test data
        X_test_poly = poly_transformer.transform(X_test)

        # Predictions
        y_pred_scaled = model.predict(X_test_poly)
        y_pred = self.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
        y_actual = self.scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()

        # Tampilkan beberapa prediksi
        print("Sample Predictions vs Actual:")
        for i in range(min(10, len(y_pred))):
            print(f"Data {i+1}: Predicted = {y_pred[i]:.2f}, Actual = {y_actual[i]:.2f}, Error = {abs(y_pred[i]-y_actual[i]):.2f}")

        return y_pred, y_actual

    def predict_new_data(self, best_model_info, new_data):
        """Memprediksi data baru yang unseen"""
        print("\n=== PREDICTIONS FOR NEW UNSEEN DATA ===\n")

        degree = best_model_info['degree']
        model = best_model_info['model']
        poly_transformer = best_model_info['poly_transformer']

        # Scale new data
        new_data_scaled = self.scaler_X.transform(new_data)

        # Transform to polynomial features
        new_data_poly = poly_transformer.transform(new_data_scaled)

        # Make predictions
        predictions_scaled = model.predict(new_data_poly)
        predictions = self.scaler_y.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()

        # Display results
        feature_names = ['Luas Tanah', 'Luas Bangunan', 'Kamar Tidur', 'Umur Bangunan', 'Jarak Pusat']

        for i, (features, pred) in enumerate(zip(new_data, predictions)):
            print(f"\nProperti {i+1}:")
            for j, feature in enumerate(feature_names):
                print(f"  {feature}: {features[j]}")
            print(f"  Predicted Harga: Rp {pred:,.2f} juta")
            print(f"  Predicted Harga: Rp {pred*1000000:,.0f}")

        return predictions

    def save_model(self, best_model_info, filename='best_property_model.pkl'):
        """Menyimpan model terbaik"""
        model_data = {
            'model': best_model_info['model'],
            'degree': best_model_info['degree'],
            'poly_transformer': best_model_info['poly_transformer'],
            'scaler_X': self.scaler_X,
            'scaler_y': self.scaler_y,
            'model_name': best_model_info['model_name'],
            'cv_score': best_model_info['cv_score']
        }

        with open(filename, 'wb') as f:
            pickle.dump(model_data, f)

        print(f"\nModel saved as {filename}")

def main():
    """Main function untuk menjalankan seluruh pipeline"""
    print("=== PROPERTY PRICE PREDICTION WITH POLYNOMIAL REGRESSION ===\n")

    # Initialize classes
    predictor = PropertyPricePredictor()
    evaluator = ModelEvaluator(predictor.scaler_y)
    regularizer = RegularizationAnalyzer(predictor.scaler_y)
    selector = ModelSelector(predictor.scaler_X, predictor.scaler_y)

    # 1. Data Preparation
    print("1. DATA PREPARATION")
    data = predictor.generate_synthetic_data(n_samples=200)
    correlation_matrix = predictor.exploratory_data_analysis(data)

    # Preprocess data
    (X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled,
     X_train, X_test, y_train, y_test) = predictor.preprocess_data(data)

    # 2. Model Implementation
    print("\n2. MODEL IMPLEMENTATION")
    model_impl = ModelImplementation(predictor.scaler_X, predictor.scaler_y)

    degrees = [1, 2, 3, 4, 5]
    poly_features = model_impl.create_polynomial_features(X_train_scaled, X_test_scaled, degrees)

    # Train models untuk setiap degree
    models = {}
    for degree in degrees:
        models[degree] = model_impl.train_models(
            poly_features[degree]['train'],
            poly_features[degree]['test'],
            y_train_scaled, y_test_scaled, degree
        )

    # Custom implementation (bonus)
    custom_result = model_impl.custom_polynomial_regression(
        X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled, degree=2
    )

    # 3. Model Evaluation
    print("\n3. MODEL EVALUATION")
    results_df = evaluator.evaluate_all_models(models, poly_features, y_train_scaled, y_test_scaled)
    evaluator.plot_model_comparison(results_df)

    # 4. Regularization Analysis
    print("\n4. REGULARIZATION ANALYSIS")
    regularization_results = regularizer.analyze_regularization(
        X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, [2, 3, 4]
    )

    feature_names = ['luas_tanah', 'luas_bangunan', 'kamar_tidur', 'umur_bangunan', 'jarak_pusat']
    regularizer.analyze_feature_importance(models, poly_features, feature_names)

    # 5. Model Selection & Prediction
    print("\n5. MODEL SELECTION & PREDICTION")
    best_model_info = selector.select_best_model(results_df, models, poly_features, X_train_scaled, y_train_scaled)

    # Plot residuals untuk model terbaik
    evaluator.plot_residuals(models, poly_features, X_test_scaled, y_test_scaled,
                           (best_model_info['degree'], best_model_info['model_name'], best_model_info['model']))

    # Final predictions
    y_pred, y_actual = selector.make_predictions(best_model_info, X_test, y_test_scaled)

    # Predict new unseen data
    new_data = np.array([
        [120, 80, 2, 5, 8],    # Properti 1
        [200, 150, 3, 2, 5],   # Properti 2
        [350, 250, 4, 1, 3],   # Properti 3
        [80, 60, 1, 10, 12],   # Properti 4
        [400, 300, 5, 0, 2]    # Properti 5
    ])

    new_predictions = selector.predict_new_data(best_model_info, new_data)

    # Save model
    selector.save_model(best_model_info)

    # 6. Generate Report
    generate_final_report(results_df, best_model_info, data, new_predictions)

def generate_final_report(results_df, best_model_info, data, new_predictions):
    """Generate final report dan insights"""
    print("\n" + "="*80)
    print("FINAL REPORT & INSIGHTS")
    print("="*80)

    print("\nEXECUTIVE SUMMARY:")
    print("• Model polynomial regression berhasil diimplementasikan untuk prediksi harga properti")
    print(f"• Model terbaik: Degree {best_model_info['degree']} - {best_model_info['model_name']}")
    print(f"• Cross-validation score: {best_model_info['cv_score']:.4f}")

    print("\nKEY INSIGHTS FROM EDA:")
    print("• Terdeteksi hubungan non-linear antara fitur dan harga properti")
    print("• Luas bangunan dan jumlah kamar tidur menunjukkan korelasi positif terkuat dengan harga")
    print("• Umur bangunan dan jarak ke pusat kota menunjukkan korelasi negatif")

    print("\nMODEL PERFORMANCE COMPARISON:")
    best_models = results_df.nlargest(5, 'Test R²')[['Degree', 'Model', 'Train R²', 'Test R²', 'Test RMSE']]
    print(best_models.to_string(index=False))

    print("\nRECOMMENDATIONS:")
    print(f"1. Polynomial Degree Terbaik: {best_model_info['degree']}")
    print(f"2. Regularization Method: {best_model_info['model_name']}")
    print("3. Gunakan feature scaling untuk meningkatkan stabilitas model")
    print("4. Monitor overfitting untuk degree > 3")

    print("\nLIMITATIONS & IMPROVEMENT SUGGESTIONS:")
    print("• Data sintetik mungkin tidak merepresentasikan real-world complexity sepenuhnya")
    print("• Model bisa diperbaiki dengan feature engineering tambahan")
    print("• Consider ensemble methods untuk improved performance")
    print("• Tambahkan more data points untuk training yang lebih robust")

    print(f"\nPREDICTIONS FOR 5 NEW PROPERTIES:")
    for i, pred in enumerate(new_predictions, 1):
        print(f"Properti {i}: Rp {pred:,.2f} juta (Rp {pred*1000000:,.0f})")

if __name__ == "__main__":
    main()